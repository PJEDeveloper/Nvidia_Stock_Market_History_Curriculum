{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives\n",
        "<font color=\"#12A80D\"> <b>• Generate next-day NVDA close using a Ridge meta-model fed by latest lookback predictions, plus a news-sentiment confidence signal.</br>• Spin up a clean Colab runtime, mount Drive, pin/install packages (Transformers, joblib, newsapi-python), and define reproducible paths for inputs, cache, and logs.</br>• Load the NewsAPI key from Drive, initialize the client, and implement a 5-day JSON cache to avoid quota issues and ensure deterministic runs.</br>• Fetch Nvidia news from the last 5 days, normalize fields (date/title/description), deduplicate, and persist to cache for reuse.</br>• Load ProsusAI/FinBERT (tokenizer + model), batch texts, compute softmax class probabilities, and aggregate avg positive/neutral/negative scores across articles.</br>• Map aggregated sentiment into a qualitative confidence label (STRONG / NEUTRAL / WEAK) based on thresholded averages.</br>• Discover the latest <code>* _ predictions.csv</code> from each lookback folder (1D–365D), extract the final row, and assemble an <code>X_input</code> row of <code>Pred _ *</code>  features.</br>• Load the saved feature column order and Ridge meta-model (joblib), align columns, and produce the ensemble predicted close for the next session.</br>• Compute and print a concise summary: last actual close, predicted close, Δ% change, and the sentiment-based confidence context.</br>• Log one row to an <code>ensemble_prediction_log.csv</code> with timestamp, prediction, Δ%, confidence, avg sentiment scores, and each base model’s latest <code>Pred _ *</code> value—ensuring stable column order for append-safe runs.</br>• Practice engineering hygiene: explicit error messages for missing keys/files, consistent timezone/date handling, idempotent caching, and clear checkpoint prints for traceability.\n",
        "\n",
        "</b> </font>"
      ],
      "metadata": {
        "id": "s0p8RXRXuuNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dependencies into the Colab Runtime Environment\n",
        "<font color=\"#12A80D\"> <b>• Installs and upgrades required Python packages in the Colab environment.</br>• Any installation errors can be ignored, as unused dependencies do not affect the execution of <code>Nvidia_Next_Day_Closing_Meta_Model_Prediction_w_API_Call_Week6.ipynb</code>.</b> </font>"
      ],
      "metadata": {
        "id": "tO1QXM-DoJe5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqNez4puI0Ls",
        "outputId": "8fff8cfe-4cfd-479e-c036-874949af1b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch==2.6.0+cu124 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision==0.21.0+cu124 in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio==2.6.0+cu124 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0+cu124)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0+cu124) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Widgets & core data stack\n",
        "!pip install --quiet ipywidgets\n",
        "!pip install --quiet pandas==2.2.2 tqdm\n",
        "# numpy==2.0.2 is already present\n",
        "\n",
        "# Option B (explicit pin, also compatible)\n",
        "!pip install -U \"torch==2.6.0+cu124\" \"torchvision==0.21.0+cu124\" \"torchaudio==2.6.0+cu124\" --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# News API + utilities\n",
        "!pip install --quiet newsapi-python==0.2.7\n",
        "!pip install --quiet joblib==1.4.2\n",
        "!pip install --quiet requests==2.32.3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary Libraries and Modules\n",
        "<font color=\"#12A80D\"> <b>• Imports essential Python libraries and modules for the project</br>• Includes file and path handling (os, glob), data manipulation (pandas), model persistence (joblib), and date/time utilities (datetime, timedelta)</br>• Loads API clients (NewsApiClient), HTTP request handling (requests), and NLP tools (transformers for model/tokenizer loading)</br>• Enables PyTorch tensor operations (torch) and JSON parsing (json) for configuration and data exchange</b> </font>"
      ],
      "metadata": {
        "id": "DHiCgdrNow45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File and path handling\n",
        "import os        # File system operations\n",
        "import glob      # File pattern matching\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd   # Data analysis and DataFrame handling\n",
        "\n",
        "# Model saving/loading\n",
        "import joblib         # Serialize and deserialize models and Python objects\n",
        "\n",
        "# Date and time utilities\n",
        "from datetime import datetime, timedelta  # Date/time handling and calculations\n",
        "\n",
        "# News API client\n",
        "from newsapi import NewsApiClient # Access to the News API service\n",
        "\n",
        "# HTTP requests\n",
        "import requests # Make HTTP requests to external APIs\n",
        "\n",
        "# NLP model loading and tokenization\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification  # Hugging Face Transformers for text classification\n",
        "\n",
        "# PyTorch deep learning framework\n",
        "import torch # Tensor operations and model inference\n",
        "\n",
        "# JSON utilities\n",
        "import json # Work with JSON data (parse, dump, load)\n"
      ],
      "metadata": {
        "id": "QeUEGZXTT9mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive in the Colab notebook to access its contents\n",
        "<font color=\"#12A80D\"> <b>• Requires granting access to Google Drive</br>• Forces remounting even if already mounted</b> </font>"
      ],
      "metadata": {
        "id": "mtnvbDiPUuRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive in Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)  # force_remount=True ensures a fresh mount"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVjUY52JUnuD",
        "outputId": "1652cf8e-71db-42c5-94ba-1bb5e73ed4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read and configure News API settings\n",
        "<font color=\"#12A80D\"> <b>• Reads the NewsAPI.org API key from a local text file stored in Google Drive</br>• Initializes a NewsApiClient instance using the loaded API key to enable authenticated requests</br>• Defines a directory path for caching retrieved news data and specifies a JSON file path (news_cache.json) to store the cached results</br>• Facilitates efficient reuse of news data by avoiding repeated API calls for previously fetched content</b> </font>"
      ],
      "metadata": {
        "id": "YiW_KQYSpPuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read NewsAPI.org API Key from local file\n",
        "with open(\"/content/drive/My Drive/Nvidia_Stock_Market_History/API_keys/newsapi_key.txt\", \"r\") as f:\n",
        "    NEWSAPI_API_KEY = f.read().strip()\n",
        "\n",
        "# Initialize NewsAPI client\n",
        "newsapi = NewsApiClient(api_key=NEWSAPI_API_KEY)\n",
        "\n",
        "# Directory where cached news data will be stored\n",
        "NEWS_CACHE_DIR = '/content/drive/My Drive/Nvidia_Stock_Market_History/Training/news_cache'\n",
        "\n",
        "# Path to JSON file containing cached news results\n",
        "NEWS_CACHE_PATH = os.path.join(NEWS_CACHE_DIR, \"news_cache.json\")\n"
      ],
      "metadata": {
        "id": "_CDlPFD0UTWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load FinBERT sentiment analysis model and tokenizer\n",
        "<font color=\"#12A80D\"> <b>• Loads the pretrained FinBERT model (ProsusAI/finbert) and its associated tokenizer from Hugging Face Transformers</br>• The tokenizer converts raw text into model-ready input IDs and attention masks</br>• The model is a sequence classification transformer fine-tuned for financial sentiment analysis (positive, negative, neutral)</br>• Enables direct inference on financial news or market-related text to assess sentiment trends for downstream stock prediction workflows</b> </font>"
      ],
      "metadata": {
        "id": "fdEhZOOopoge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FinBERT financial sentiment analysis model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")  # Tokenizer to convert text into model input IDs\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")  # Pretrained FinBERT model for sentiment classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXJqNkJgWBkX",
        "outputId": "3fb8232c-f0db-475b-ffa4-c16e72e43ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meta-model prediction configuration\n",
        "<font color=\"#12A80D\"> <b>• Defines key file paths and directories required for generating and logging ensemble predictions</br>• Specifies the trained Ridge meta-model file location (META_MODEL_PATH) used for inference</br>• Sets the log directory (LOG_DIR_PRED) for storing prediction history and outputs</br>• Creates a full log file path (LOG_PATH) for saving ensemble prediction results as a CSV</br>• Ensures consistent file organization for reproducibility and future evaluation</b> </font>"
      ],
      "metadata": {
        "id": "8eLCcegOqFYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Configuration\n",
        "# ---------------------------\n",
        "\n",
        "# Path to the trained Ridge meta-model file\n",
        "META_MODEL_PATH = \"/content/drive/My Drive/Nvidia_Stock_Market_History/Training/Meta_Model_Trained/meta_model_ridge.joblib\"\n",
        "\n",
        "# Directory for storing prediction logs\n",
        "LOG_DIR_PRED = \"/content/drive/My Drive/Nvidia_Stock_Market_History/Training/Meta_Model_Trained\"\n",
        "\n",
        "# Full path for the ensemble prediction log CSV\n",
        "LOG_PATH = os.path.join(LOG_DIR_PRED, \"ensemble_prediction_log.csv\")"
      ],
      "metadata": {
        "id": "dIhWNiz3ZPlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load latest lookback predictions\n",
        "<font color=\"#12A80D\"> <b>• Iterates through all defined lookback periods (LOOKBACK_PERIODS) to find the most recent prediction outputs for each timeframe</br>• Uses folder naming conventions containing date and time stamps to sort and identify the latest run within each lookback folder</br>• Searches for *_predictions.csv files in the most recent subfolder and records their paths for downstream processing</br>• Ensures the meta-model always uses the freshest predictions from all base models without manual file selection</b> </font>"
      ],
      "metadata": {
        "id": "L9ExOjXFqZLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Load latest predictions\n",
        "# ---------------------------\n",
        "\n",
        "# Lookback folders to process\n",
        "LOOKBACK_PERIODS = [\"365D\", \"270D\", \"180D\", \"90D\", \"60D\", \"30D\", \"14D\", \"1D\"]\n",
        "\n",
        "\n",
        "# Base directory containing all lookback folders\n",
        "ENSEMBLE_INPUTS_BASE = \"/content/drive/My Drive/Nvidia_Stock_Market_History/Training/ensemble_inputs\"\n",
        "\n",
        "# Will store paths to the latest *_predictions.csv from each lookback folder\n",
        "csv_files = []\n",
        "\n",
        "for lookback in LOOKBACK_PERIODS:\n",
        "    lookback_path = os.path.join(ENSEMBLE_INPUTS_BASE, lookback)\n",
        "    # Get all subfolders inside the current lookback folder\n",
        "    subfolders = [f for f in glob.glob(os.path.join(lookback_path, \"*\")) if os.path.isdir(f)]\n",
        "    if not subfolders:\n",
        "        raise FileNotFoundError(f\"No subfolders found in {lookback_path}\")\n",
        "\n",
        "    # Parse datetime from subfolder names\n",
        "    def extract_dt(folder_name):\n",
        "        # Expects last part of name to be date_time: 2025-07-08_14-43-29\n",
        "        dt_str = folder_name.split(\"_\")[-2] + \" \" + folder_name.split(\"_\")[-1]\n",
        "        return datetime.strptime(dt_str, \"%Y-%m-%d %H-%M-%S\")\n",
        "\n",
        "    # Sort subfolders by datetime descending order\n",
        "    subfolders_sorted = sorted(subfolders, key=lambda f: extract_dt(os.path.basename(f)), reverse=True)\n",
        "    latest_subfolder = subfolders_sorted[0]\n",
        "\n",
        "    # Find the *_predictions.csv file inside the latest subfolder\n",
        "    pred_files = glob.glob(os.path.join(latest_subfolder, \"*_predictions.csv\"))\n",
        "    if not pred_files:\n",
        "        raise FileNotFoundError(f\"No *_predictions.csv found in {latest_subfolder}\")\n",
        "\n",
        "    # Assuming only one predictions file per folder\n",
        "    csv_files.append(pred_files[0])\n",
        "\n",
        "# Print the discovered prediction files\n",
        "print(\"Found prediction files:\")\n",
        "for f in csv_files:\n",
        "    print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7IS6gCLZ0hI",
        "outputId": "8dd20c24-afae-4ac8-d042-58eca2f485be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found prediction files:\n",
            "/content/drive/My Drive/Nvidia_Stock_Market_History/Training/ensemble_inputs/365D/Nvidia_Stock_Training_365D_SA_2025-08-13_04-56-15/Nvidia_C1D64_BiG550_BiG350_BAtt_D1_Lookback365_predictions.csv\n",
            "/content/drive/My Drive/Nvidia_Stock_Market_History/Training/ensemble_inputs/270D/Nvidia_Stock_Training_270D_SA_2025-08-13_02-24-48/Nvidia_C1D64_BiG250_BiG250_BiG250_BAtt_D1_Lookback270_predictions.csv\n",
            "/content/drive/My Drive/Nvidia_Stock_Market_History/Training/ensemble_inputs/180D/Nvidia_Stock_Training_180D_SA_2025-08-13_02-55-32/Nvidia_C1D64_BiG250_BiG250_BiG250_BAtt_D1_Lookback180_predictions.csv\n",
            "/content/drive/My Drive/Nvidia_Stock_Market_History/Training/ensemble_inputs/90D/Nvidia_Stock_Training_90D_SA_2025-08-13_03-13-06/Nvidia_C1D64_BiG250_BiG250_BAtt_D1_Lookback90_predictions.csv\n",
            "/content/drive/My Drive/Nvidia_Stock_Market_History/Training/ensemble_inputs/60D/Nvidia_Stock_Training_60D_SA_2025-08-12_05-01-55/Nvidia_C1D64_BiG250_BiG250_BAtt_D1_Lookback60_predictions.csv\n",
            "/content/drive/My Drive/Nvidia_Stock_Market_History/Training/ensemble_inputs/30D/Nvidia_Stock_Training_30D_SA_2025-08-12_05-19-08/Nvidia_C1D64_BiG250_BiG250_BAtt_D1_Lookback30_predictions.csv\n",
            "/content/drive/My Drive/Nvidia_Stock_Market_History/Training/ensemble_inputs/14D/Nvidia_Stock_Training_14D_SA_2025-08-12_05-32-18/Nvidia_C1D64_BiG250_BiG250_BAtt_D1_Lookback14_predictions.csv\n",
            "/content/drive/My Drive/Nvidia_Stock_Market_History/Training/ensemble_inputs/1D/Nvidia_Stock_Training_1D_SA_2025-08-11_01-59-32/Nvidia_BiG250_BiG250_BAtt_D1_Lookback1_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare latest base-model predictions for meta-model\n",
        "<font color=\"#12A80D\"> <b>• Iterates through the most recent prediction CSVs for each lookback period, extracts the final row (latest prediction), and stores the predicted close price under keys like Pred_30</br>• Combines all extracted predictions into a single DataFrame (X_input) representing the current input vector for the meta-model</br>• Loads the saved feature_cols.joblib to ensure the DataFrame columns are ordered exactly as the meta-model was trained, filling any missing values with NaN</br>• Prints the resulting input DataFrame so that the user can verify the values before running the ensemble prediction</b> </font>"
      ],
      "metadata": {
        "id": "AsHgNePOqvcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Dictionary to store latest predictions keyed by lookback period\n",
        "latest_preds = {}\n",
        "\n",
        "for path in csv_files:\n",
        "    # Extract lookback period from filename (e.g., \"Lookback30\" → \"30\")\n",
        "    lookback = [s for s in os.path.basename(path).split(\"_\") if \"Lookback\" in s][0].replace(\"Lookback\", \"\")\n",
        "    # Read the CSV for this lookback model\n",
        "    df = pd.read_csv(path)\n",
        "    # Get the last row — assumed to be the most recent prediction\n",
        "    last_row = df.iloc[-1]\n",
        "    # Extract predicted and actual close prices\n",
        "    pred_close = last_row[\"Predicted_Close\"]\n",
        "    actual_close = last_row[\"Actual_Close\"]\n",
        "    # Store the prediction under a column name format like \"Pred_30\"\n",
        "    latest_preds[f\"Pred_{lookback}\"] = pred_close\n",
        "\n",
        "# Create DataFrame for meta-model input using the collected predictions\n",
        "X_input = pd.DataFrame([latest_preds])\n",
        "\n",
        "# ---------------------------\n",
        "# Load feature columns and reindex\n",
        "# ---------------------------\n",
        "\n",
        "# Path where meta-model and related files are stored\n",
        "META_MODEL_SAVE_FOLDER = os.path.dirname(META_MODEL_PATH)\n",
        "# Load the saved feature column order\n",
        "feature_cols = joblib.load(os.path.join(META_MODEL_SAVE_FOLDER, \"feature_cols.joblib\"))\n",
        "\n",
        "# Reindex X_input to ensure correct column order and fill any missing columns with NaN\n",
        "X_input = X_input.reindex(columns=feature_cols)\n",
        "\n",
        "# Display latest predictions from the base models\n",
        "print(\"\\nLatest predictions from base models:\")\n",
        "print(X_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV4oHr7UaEK4",
        "outputId": "4ba7c61f-8c58-445b-db59-b306e229ef02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Latest predictions from base models:\n",
            "    Pred_365   Pred_270    Pred_180   Pred_90     Pred_60   Pred_30  \\\n",
            "0  182.88945  130.47496  113.420494  141.9292  127.298706  150.2968   \n",
            "\n",
            "     Pred_14     Pred_1  \n",
            "0  144.50777  146.35782  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch latest Nvidia news articles from past 5 days\n",
        "<font color=\"#12A80D\"> <b>• Uses the NewsAPI client to query for English-language news articles mentioning \"Nvidia\" within the last 5 days</br>• Formats the from_param and to parameters as YYYY-MM-DD for the API call, restricting the result set to a maximum of 30 most recent articles</br>• Parses the returned JSON, extracting only the date, title, and description for each article into a simplified list of dictionaries</br>• Returns this list so it can be processed, cached, or used for sentiment analysis by downstream functions</b> </font>"
      ],
      "metadata": {
        "id": "jjvlGli4rLsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_today_articles():\n",
        "    # Import datetime utilities locally to keep scope clean\n",
        "    from datetime import datetime, timedelta\n",
        "\n",
        "    # Get current UTC date and time\n",
        "    today = datetime.utcnow()\n",
        "    # Format today's date as YYYY-MM-DD for API query\n",
        "    today_str = today.strftime(\"%Y-%m-%d\")\n",
        "    # Get the 5 previous dates by subtracting 5 day\n",
        "    days_str = (today - timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Fetch articles from NewsAPI:\n",
        "    # - Query for Nvidia\n",
        "    # - Restrict to English-language articles\n",
        "    # - Sort by publication date (newest first)\n",
        "    # - Search only within the last 5 days\n",
        "    # - Limit results to 30 articles\n",
        "    all_articles = newsapi.get_everything(\n",
        "        q=\"Nvidia\",\n",
        "        language=\"en\",\n",
        "        sort_by=\"publishedAt\",\n",
        "        from_param=days_str,\n",
        "        to=today_str,\n",
        "        page_size=30\n",
        "    )\n",
        "\n",
        "    articles = all_articles.get(\"articles\", [])\n",
        "\n",
        "    # Convert articles to simpler dicts\n",
        "    simplified = []\n",
        "    for art in articles:\n",
        "        simplified.append({\n",
        "            \"date\": today_str,\n",
        "            \"title\": art.get(\"title\", \"\"),\n",
        "            \"description\": art.get(\"description\", \"\")\n",
        "        })\n",
        "\n",
        "    return simplified"
      ],
      "metadata": {
        "id": "VeXYv2wxaK5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cache and load recent Nvidia news articles\n",
        "<font color=\"#12A80D\"> <b>• Ensures the news cache directory exists and attempts to load previously cached news data from a JSON file</br>• If today's date is missing from the cache, calls <code>fetch_today_articles()</code> to retrieve the latest Nvidia news from NewsAPI and adds it to the cache</br>• Maintains only the most recent 5 days of news data to limit file size and improve lookup speed</br>• Writes the updated cache back to disk, then flattens all stored days into a single consolidated article list for downstream sentiment analysis</b> </font>"
      ],
      "metadata": {
        "id": "3Y-Jy1sxrl0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the cache directory exists\n",
        "os.makedirs(NEWS_CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# Load an existing cache file if it exists, otherwise start fresh\n",
        "if os.path.exists(NEWS_CACHE_PATH):\n",
        "    with open(NEWS_CACHE_PATH, \"r\") as f:\n",
        "        news_cache = json.load(f)\n",
        "else:\n",
        "    news_cache = {}\n",
        "\n",
        "# Get a sorted list of all cached dates (oldest → newest)\n",
        "existing_dates = sorted(news_cache.keys())\n",
        "\n",
        "# Today's date string (local time) for indexing in the cache\n",
        "today_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# If today's date is not in the cache, fetch new articles\n",
        "if today_str not in existing_dates:\n",
        "    print(\"Fetching today's news articles...\")\n",
        "    todays_articles = fetch_today_articles() # Pull from NewsAPI\n",
        "    news_cache[today_str] = todays_articles  # Add to cache\n",
        "\n",
        "     # Keep cache size small — only store the latest 5 days of articles\n",
        "    all_dates_sorted = sorted(news_cache.keys(), reverse=True) # newest → oldest\n",
        "    trimmed_dates = all_dates_sorted[:5]\n",
        "    news_cache = {date: news_cache[date] for date in trimmed_dates}\n",
        "\n",
        "    # Save updated cache back to disk\n",
        "    with open(NEWS_CACHE_PATH, \"w\") as f:\n",
        "        json.dump(news_cache, f, indent=2)\n",
        "\n",
        "else:\n",
        "    print(\"Today's articles already in cache.\")\n",
        "\n",
        "\n",
        "# Flatten the cache: combine all articles across stored days into a single list\n",
        "all_articles = []\n",
        "for date in sorted(news_cache.keys()):\n",
        "    all_articles.extend(news_cache[date])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdlB7c2DaPiw",
        "outputId": "1588ae04-a924-4b75-dfb8-c9f27f703030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching today's news articles...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute FinBERT sentiment over last 5 days of news\n",
        "<font color=\"#12A80D\"> <b>• Combines each article’s title and description, skipping empty entries</br>• Tokenizes text with the FinBERT tokenizer and runs inference with gradients disabled for efficiency</br>• Applies softmax to model logits to obtain Negative, Neutral, and Positive class probabilities</br>• Aggregates probabilities across all articles and computes average sentiment scores</br>• Defaults to a neutral profile (Positive=0.0, Neutral=1.0, Negative=0.0) if no articles are available</br>• Maps averaged scores to a qualitative confidence label: STRONG if avg_positive ≥ 0.5, WEAK if avg_negative ≥ 0.5, otherwise NEUTRAL</b> </font>"
      ],
      "metadata": {
        "id": "66QDFZTmr7Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nComputing sentiment over last 5 days of news...\")\n",
        "\n",
        "# Lists to store probability scores for each sentiment class\n",
        "all_pos = []\n",
        "all_neu = []\n",
        "all_neg = []\n",
        "\n",
        "# Loop over every cached news article\n",
        "for article in all_articles:\n",
        "    # Combine title and description into one text string\n",
        "    text = (article.get(\"title\") or \"\") + \". \" + (article.get(\"description\") or \"\")\n",
        "    if not text.strip(): # Skip empty entries\n",
        "        continue\n",
        "\n",
        "    # Tokenize text for FinBERT (returns PyTorch tensors, truncates if too long)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    # Run model inference without gradient tracking\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Convert raw logits to probabilities via softmax\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]\n",
        "\n",
        "    # Append each class probability to the correct list\n",
        "    all_neg.append(probs[0].item()) # Negative sentiment score\n",
        "    all_neu.append(probs[1].item()) # Neutral sentiment score\n",
        "    all_pos.append(probs[2].item()) # Positive sentiment score\n",
        "\n",
        "# If we computed at least one sentiment, calculate averages\n",
        "if all_pos:\n",
        "    avg_positive = sum(all_pos) / len(all_pos)\n",
        "    avg_neutral = sum(all_neu) / len(all_neu)\n",
        "    avg_negative = sum(all_neg) / len(all_neg)\n",
        "else:\n",
        "    # No articles → default to neutral sentiment\n",
        "    avg_positive = 0.0\n",
        "    avg_neutral = 1.0\n",
        "    avg_negative = 0.0\n",
        "\n",
        "# Map average sentiment scores to a qualitative confidence label\n",
        "if avg_positive >= 0.5:\n",
        "    confidence = \"STRONG\" # Market sentiment strongly positive\n",
        "elif avg_negative >= 0.5:\n",
        "    confidence = \"WEAK\" # Market sentiment strongly negative\n",
        "else:\n",
        "    confidence = \"NEUTRAL\" # No strong directional sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKRCDA3OaR7e",
        "outputId": "506465ff-59eb-4c2e-8748-2d0dcbb5977f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing sentiment over last 5 days of news...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Ridge meta-model and generate ensemble prediction\n",
        "<font color=\"#12A80D\"> <b>• Loads the pre-trained Ridge regression meta-model from disk using joblib</br>• Uses the latest base-model predictions in <code>X_input</code> (ensuring correct feature ordering) as input</br>• Generates a single ensemble prediction representing the forecasted next-day stock closing price</br>• This meta-model combines multiple lookback models to improve predictive accuracy over individual models</b> </font>"
      ],
      "metadata": {
        "id": "HUlVu7Bysl4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Load Ridge meta-model\n",
        "# ---------------------------\n",
        "\n",
        "# Load the trained Ridge regression model from disk\n",
        "# This meta-model was trained to combine predictions from all base models\n",
        "meta_model = joblib.load(META_MODEL_PATH)\n",
        "\n",
        "# Predict ensemble output\n",
        "# X_input contains the latest predictions from each base model (properly ordered)\n",
        "# The meta-model predicts the final next-day stock closing price\n",
        "ensemble_pred = meta_model.predict(X_input.values)[0]"
      ],
      "metadata": {
        "id": "9-0kYntcaXGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display ensemble prediction summary\n",
        "<font color=\"#12A80D\"> <b>Calculates the percentage change between the ensemble-predicted next-day closing price and today’s actual closing price</br>Prints a clear prediction report showing:</br>• Forecasted next-day close</br>• Today’s actual close</br>• Percentage change</br>• Sentiment-based confidence label</br>• Average positive, neutral, and negative sentiment scores from the past five days of news</br>Provides an interpretable snapshot of the prediction and its sentiment context for decision-making</b> </font>"
      ],
      "metadata": {
        "id": "1Y4ia2Ens52K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Display result\n",
        "# ---------------------------\n",
        "\n",
        "# Calculate the percentage difference between the predicted next-day close\n",
        "# and today's actual close price\n",
        "delta_pct = (ensemble_pred - actual_close) / actual_close * 100\n",
        "\n",
        "# Print prediction summary\n",
        "print(\"\\n--------------------------------------------\")\n",
        "print(f\"Predicted Close for Next Day: ${ensemble_pred:.2f}\")\n",
        "print(f\"Today's Close: ${actual_close:.2f}\")\n",
        "print(f\"Predicted Change: {delta_pct:.2f}%\")\n",
        "print(f\"Confidence: {confidence}\")\n",
        "print(f\"Sentiment: Pos={avg_positive:.2f}, Neu={avg_neutral:.2f}, Neg={avg_negative:.2f}\")\n",
        "print(\"--------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WDddLX3aZF_",
        "outputId": "3514530a-2033-4373-d18f-038e30c70d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------\n",
            "Predicted Close for Next Day: $176.99\n",
            "Today's Close: $182.70\n",
            "Predicted Change: -3.12%\n",
            "Confidence: NEUTRAL\n",
            "Sentiment: Pos=0.35, Neu=0.35, Neg=0.30\n",
            "--------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log ensemble prediction with metadata to CSV\n",
        "<font color=\"#12A80D\"> <b>• Captures current date and timestamp for audit fields (<code>Date_Predicted</code>, <code>Timestamp</code>)</br>• Assembles a one-row log entry including ensemble prediction, last close, percent change, confidence label, and average FinBERT sentiment scores</br>• Adds latest base-model predictions (<code>Pred_365</code>, <code>Pred_270</code>, …) to the same entry</br>• Checks if the log file exists and reorders columns to match existing headers for consistency</br>• Appends the entry to <code>ensemble_prediction_log.csv</code> if present, otherwise creates a new file with headers</br>• Prints a status message indicating whether the log was appended or created</b> </font>"
      ],
      "metadata": {
        "id": "Y82jDqBxtKZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Current date and time\n",
        "now = datetime.now()\n",
        "today = now.strftime(\"%Y-%m-%d\") # Just the date for \"Date_Predicted\" column\n",
        "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\") # Full timestamp for detailed logging\n",
        "\n",
        "# Prepare log data dictionary\n",
        "log_data = {\n",
        "    \"Date_Predicted\": [today], # Prediction date\n",
        "    \"Timestamp\": [timestamp], # Exact time of prediction\n",
        "    \"Ensemble_Predicted_Close\": [ensemble_pred], # Meta-model predicted close price\n",
        "    \"Last_Close\": [actual_close],  # Actual latest close price\n",
        "    \"Predicted_Change_Percent\": [delta_pct],   # % change from last close to predicted close\n",
        "    \"Confidence\": [confidence], # Sentiment-based confidence label\n",
        "    \"Avg_Positive\": [avg_positive], # Avg FinBERT positive sentiment score\n",
        "    \"Avg_Neutral\": [avg_neutral],  # Avg FinBERT neutral sentiment score\n",
        "    \"Avg_Negative\": [avg_negative]  # Avg FinBERT negative sentiment score\n",
        "}\n",
        "\n",
        "# Add base model latest predictions to the log (Pred_365, Pred_270, etc.)\n",
        "log_data.update({k: [v] for k, v in latest_preds.items()})\n",
        "\n",
        "# Convert log_data to a DataFrame (one row for this run)\n",
        "log_entry = pd.DataFrame(log_data)\n",
        "\n",
        "# Determine if log file already exists\n",
        "file_exists = os.path.exists(LOG_PATH)\n",
        "\n",
        "# If appending, ensure consistent column order\n",
        "if file_exists:\n",
        "    # Read just headers\n",
        "    headers = pd.read_csv(LOG_PATH, nrows=0).columns.tolist()\n",
        "    # Reorder columns to match\n",
        "    log_entry = log_entry[headers]\n",
        "\n",
        "# Append to log file if it exists, else create a new file with headers\n",
        "log_entry.to_csv(LOG_PATH, mode=\"a\", header=not file_exists, index=False)\n",
        "\n",
        "\n",
        "# Status message\n",
        "if file_exists:\n",
        "    print(f\"\\nPrediction appended to existing log: {LOG_PATH}\")\n",
        "else:\n",
        "    print(f\"\\nNew log file created: {LOG_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfVWJMugahSs",
        "outputId": "f0ea4a78-662d-402d-e3c9-d72bbe971a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New log file created: /content/drive/My Drive/Nvidia_Stock_Market_History/Training/Meta_Model_Trained/ensemble_prediction_log.csv\n"
          ]
        }
      ]
    }
  ]
}